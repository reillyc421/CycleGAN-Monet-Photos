{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30299,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\nThe purpose of this notebook is to apply Cycle GAN to translate photos into the style of Monet paintings. This task does not have paired images (the same image as a photo and Monet painting), so the discriminator component of a GAN is necessary. The generator will create photos in the style of Monet paintings and the discriminator will evaluate how well those generated paintings pass for true Monet paintings.\nIn order to ensure the generated paintings not only resemble Monet paintings, but also resemble the original photograph, the model will also be evaluated on how well the generated paintings can be translated back into photographs. This is the 'cycle' aspect of the Cycle GAN because the photograph will be translated to a Monet painting and then translated back to a photograph. The loss will be calculated for both directions of the cycle.","metadata":{}},{"cell_type":"markdown","source":"### Acknowledgements\nThis notebook is based on Amy Jang's tutorial notebook found here: https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-11-01T15:45:34.671887Z","iopub.execute_input":"2024-11-01T15:45:34.672329Z","iopub.status.idle":"2024-11-01T15:45:41.399373Z","shell.execute_reply.started":"2024-11-01T15:45:34.672232Z","shell.execute_reply":"2024-11-01T15:45:41.397791Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of replicas: 1\n2.6.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Load data\n\nThe dataset consists of 300 Monet paintings and 7028 photographs. All images have a consistent size of 256x256. The images are available in JPEG or TFRecord formats. This project will use the TFRecord format.","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:45:53.628005Z","iopub.execute_input":"2024-11-01T15:45:53.628628Z","iopub.status.idle":"2024-11-01T15:45:54.143873Z","shell.execute_reply.started":"2024-11-01T15:45:53.628596Z","shell.execute_reply":"2024-11-01T15:45:54.142367Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:45:55.437871Z","iopub.execute_input":"2024-11-01T15:45:55.438859Z","iopub.status.idle":"2024-11-01T15:45:58.197967Z","shell.execute_reply.started":"2024-11-01T15:45:55.438772Z","shell.execute_reply":"2024-11-01T15:45:58.196699Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Monet TFRecord Files: 5\nPhoto TFRecord Files: 20\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Process Images\nThe images are a consistent size (256 x 256). The image size variable is set outside the function in the event the model will be used with photographs outside the given dataset. The variable could be changed to match the size of those photographs. The below functions will set the channels to 3 (RGB images) and rescale the images to [-1, 1].\n","metadata":{}},{"cell_type":"code","source":"image_size = [256, 256]\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels = 3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*image_size, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:46:47.849247Z","iopub.execute_input":"2024-11-01T15:46:47.849642Z","iopub.status.idle":"2024-11-01T15:46:47.858377Z","shell.execute_reply.started":"2024-11-01T15:46:47.849612Z","shell.execute_reply":"2024-11-01T15:46:47.856634Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Extract images from datasets","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled = True, ordered = False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:46:52.517996Z","iopub.execute_input":"2024-11-01T15:46:52.518368Z","iopub.status.idle":"2024-11-01T15:46:52.525272Z","shell.execute_reply.started":"2024-11-01T15:46:52.518333Z","shell.execute_reply":"2024-11-01T15:46:52.523911Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled = True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled = True).batch(1)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T15:46:54.040750Z","iopub.execute_input":"2024-11-01T15:46:54.041527Z","iopub.status.idle":"2024-11-01T15:46:54.384604Z","shell.execute_reply.started":"2024-11-01T15:46:54.041490Z","shell.execute_reply":"2024-11-01T15:46:54.383490Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Sample images from each dataset\nexample_monet = next(iter(monet_ds))\nexample_photo = next(iter(photo_ds))\n\nplt.subplot(121)\nplt.title('Photo')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(example_monet[0] * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:01:53.979398Z","iopub.execute_input":"2024-10-31T22:01:53.979820Z","iopub.status.idle":"2024-10-31T22:01:58.090313Z","shell.execute_reply.started":"2024-10-31T22:01:53.979786Z","shell.execute_reply":"2024-10-31T22:01:58.089419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build the Architectures\n\n## Generator\n\nThe generator will be a U-Net model. This model involves two steps: contracting path and expansive path.\n- Contracting path (downsample function): extracts features using a convolution layer, an instance normalization layer, and a LeakyReLU activation function.\n- Expansive path (upsample function): translates features into learned Monet-style using an inverse convolution layer, an instance normalization layer, and a LeakyReLU activation function.\nI performed informal comparisons of activation functions and chose to use Leaky Relu for both paths.","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n\n    model = keras.Sequential()\n    model.add(layers.Conv2D(filters, \n                            size, \n                            strides = 2, \n                            padding = 'same',\n                             kernel_initializer = initializer,\n                            use_bias = False))\n    model.add(tfa.layers.InstanceNormalization(gamma_initializer = gamma_init))\n    model.add(layers.LeakyReLU())\n\n    return model\n\ndef upsample(filters, size):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n\n    model = keras.Sequential()\n    model.add(layers.Conv2DTranspose(filters, \n                                     size, \n                                     strides = 2,\n                                      padding = 'same',\n                                      kernel_initializer = initializer,\n                                      use_bias = False))\n    model.add(tfa.layers.InstanceNormalization(gamma_initializer = gamma_init))\n    model.add(layers.LeakyReLU())\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:00.904584Z","iopub.execute_input":"2024-10-31T22:02:00.905311Z","iopub.status.idle":"2024-10-31T22:02:00.915423Z","shell.execute_reply.started":"2024-10-31T22:02:00.905277Z","shell.execute_reply":"2024-10-31T22:02:00.914394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generator architecture\nThe up and down stacks mirror each other such that each feature reduction in the down stack has a matching expansion in the up stack.","metadata":{}},{"cell_type":"code","source":"# Use the downsample and upsample functions to build the generator\ndef Generator():\n    inputs = layers.Input(shape=[256, 256, 3])\n\n    down_stack = [\n        downsample(64, 4), # (128, 128, 64)\n        downsample(128, 4), # (64, 64, 128)\n        downsample(256, 4), # (32, 32, 256)\n        downsample(512, 4), # (16, 16, 512)\n        downsample(512, 4), # (8, 8, 512)\n        downsample(512, 4), # (4, 4, 512)\n        downsample(512, 4), # (2, 2, 512)\n        downsample(512, 4), # (1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4), # (2, 2, 1024)\n        upsample(512, 4), # (4, 4, 1024)\n        upsample(512, 4), # (8, 8, 1024)\n        upsample(512, 4), # (16, 16, 1024)\n        upsample(256, 4), # (32, 32, 512)\n        upsample(128, 4), # (64, 64, 256)\n        upsample(64, 4), # (128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides = 2,\n                                  padding = 'same',\n                                  kernel_initializer = initializer,\n                                  activation = 'tanh') # (256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs = inputs, outputs = x)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:04.157175Z","iopub.execute_input":"2024-10-31T22:02:04.157531Z","iopub.status.idle":"2024-10-31T22:02:04.169695Z","shell.execute_reply.started":"2024-10-31T22:02:04.157501Z","shell.execute_reply":"2024-10-31T22:02:04.168715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Discriminator\n\nThe discriminator downsamples the image, then applies a convolutional layer, normalization layer, activation function, and a final convolutional layer. ","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n\n    inp = layers.Input(shape = [256, 256, 3], name = 'input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides = 1,\n                         kernel_initializer = initializer,\n                         use_bias = False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer = gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides = 1,\n                         kernel_initializer = initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs = inp, outputs = last)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:08.480350Z","iopub.execute_input":"2024-10-31T22:02:08.480742Z","iopub.status.idle":"2024-10-31T22:02:08.490614Z","shell.execute_reply.started":"2024-10-31T22:02:08.480708Z","shell.execute_reply":"2024-10-31T22:02:08.489516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = Generator() # generates Monet-style paintings\n    photo_generator = Generator() # generates photographs\n\n    monet_discriminator = Discriminator() # discriminates between true Monets and false Monets\n    photo_discriminator = Discriminator() # discriminates between true photographs and false photographs","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:13.707508Z","iopub.execute_input":"2024-10-31T22:02:13.708406Z","iopub.status.idle":"2024-10-31T22:02:16.013824Z","shell.execute_reply.started":"2024-10-31T22:02:13.708367Z","shell.execute_reply":"2024-10-31T22:02:16.012985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cycle GAN Model\n\nThe class below creates a keras model from the generators and discriminators. This allows the use of keras model methods to train the model.","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # Photo -> Monet -> Photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # Monet -> Photo -> Monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # Monet -> Monet\n            same_monet = self.m_gen(real_monet, training=True)\n            \n            # Photo -> Photo\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # Pass true Monet to Monet discriminator\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            \n            # Pass true photo to photo discriminator\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # Pass fake Monet to Monet discriminator\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            \n            # Pass fake photo to photo discriminator\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # Generator losses\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # Consistency losses for Monet and photo\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # Total generator losses\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # Discriminator losses\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:18.510193Z","iopub.execute_input":"2024-10-31T22:02:18.510586Z","iopub.status.idle":"2024-10-31T22:02:18.532302Z","shell.execute_reply.started":"2024-10-31T22:02:18.510549Z","shell.execute_reply":"2024-10-31T22:02:18.531319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Functions\n\nA perfect generator will produce images that the discriminator considers real. The discriminator loss function outputs 1s for real images; therefore, a perfect generator will cause the discriminator to output 1s. The consistency loss calculates the loss between the original photo and the photo that has been translated to Monet style and back. In a perfect system, the translated photo would perfectly match the original photo. The identity loss calculates the loss between a photo that is passed through the photo generator. It should not be changed because it is already a photo.\n","metadata":{}},{"cell_type":"code","source":"# Discriminator loss\nwith strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction = tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction = tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5\n    \n# Generator loss\nwith strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n    \n# Consistency loss\nwith strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1\n    \n# Identity loss\nwith strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:02:27.577243Z","iopub.execute_input":"2024-10-31T22:02:27.577650Z","iopub.status.idle":"2024-10-31T22:02:27.588559Z","shell.execute_reply.started":"2024-10-31T22:02:27.577606Z","shell.execute_reply":"2024-10-31T22:02:27.587500Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training\n\nThe model will first be trained using the ADAM optimizer with a learning rate of 2e-4. Next, I will reduce the learning rate by half and evaluate the effect on performance.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4, beta_1 = 0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4, beta_1 = 0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4, beta_1 = 0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4, beta_1 = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:03:02.155058Z","iopub.execute_input":"2024-10-31T22:03:02.155976Z","iopub.status.idle":"2024-10-31T22:03:02.162868Z","shell.execute_reply.started":"2024-10-31T22:03:02.155935Z","shell.execute_reply":"2024-10-31T22:03:02.161780Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:03:05.399427Z","iopub.execute_input":"2024-10-31T22:03:05.400157Z","iopub.status.idle":"2024-10-31T22:03:05.415429Z","shell.execute_reply.started":"2024-10-31T22:03:05.400122Z","shell.execute_reply":"2024-10-31T22:03:05.414662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mod1_history = cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs = 25\n) ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:53:52.594902Z","iopub.execute_input":"2024-10-31T19:53:52.595200Z","iopub.status.idle":"2024-10-31T20:27:56.125877Z","shell.execute_reply.started":"2024-10-31T19:53:52.595171Z","shell.execute_reply":"2024-10-31T20:27:56.124869Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decrease learning rate\nwith strategy.scope():\n    monet_generator_optimizer2 = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.5)\n    photo_generator_optimizer2 = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.5)\n\n    monet_discriminator_optimizer2 = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.5)\n    photo_discriminator_optimizer2 = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:24:37.915430Z","iopub.execute_input":"2024-10-31T21:24:37.916377Z","iopub.status.idle":"2024-10-31T21:24:37.923377Z","shell.execute_reply.started":"2024-10-31T21:24:37.916326Z","shell.execute_reply":"2024-10-31T21:24:37.922445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model2 = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model2.compile(\n        m_gen_optimizer = monet_generator_optimizer2,\n        p_gen_optimizer = photo_generator_optimizer2,\n        m_disc_optimizer = monet_discriminator_optimizer2,\n        p_disc_optimizer = photo_discriminator_optimizer2,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:24:41.015447Z","iopub.execute_input":"2024-10-31T21:24:41.015869Z","iopub.status.idle":"2024-10-31T21:24:41.032251Z","shell.execute_reply.started":"2024-10-31T21:24:41.015834Z","shell.execute_reply":"2024-10-31T21:24:41.031230Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mod2_history = cycle_gan_model2.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs = 25\n) ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:24:44.307270Z","iopub.execute_input":"2024-10-31T21:24:44.307934Z","iopub.status.idle":"2024-10-31T21:58:36.860838Z","shell.execute_reply.started":"2024-10-31T21:24:44.307897Z","shell.execute_reply":"2024-10-31T21:58:36.859967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Increase learning rate\nwith strategy.scope():\n    monet_generator_optimizer3 = tf.keras.optimizers.Adam(learning_rate = 4e-4, beta_1 = 0.5)\n    photo_generator_optimizer3 = tf.keras.optimizers.Adam(learning_rate = 4e-4, beta_1 = 0.5)\n\n    monet_discriminator_optimizer3 = tf.keras.optimizers.Adam(learning_rate = 4e-4, beta_1 = 0.5)\n    photo_discriminator_optimizer3 = tf.keras.optimizers.Adam(learning_rate = 4e-4, beta_1 = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:03:14.989929Z","iopub.execute_input":"2024-10-31T22:03:14.990915Z","iopub.status.idle":"2024-10-31T22:03:14.997539Z","shell.execute_reply.started":"2024-10-31T22:03:14.990875Z","shell.execute_reply":"2024-10-31T22:03:14.996622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model3 = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model3.compile(\n        m_gen_optimizer = monet_generator_optimizer3,\n        p_gen_optimizer = photo_generator_optimizer3,\n        m_disc_optimizer = monet_discriminator_optimizer3,\n        p_disc_optimizer = photo_discriminator_optimizer3,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:03:17.522314Z","iopub.execute_input":"2024-10-31T22:03:17.523202Z","iopub.status.idle":"2024-10-31T22:03:17.539064Z","shell.execute_reply.started":"2024-10-31T22:03:17.523160Z","shell.execute_reply":"2024-10-31T22:03:17.538303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mod3_history = cycle_gan_model3.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs = 25\n) ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:03:19.296441Z","iopub.execute_input":"2024-10-31T22:03:19.296816Z","iopub.status.idle":"2024-10-31T22:36:53.416583Z","shell.execute_reply.started":"2024-10-31T22:03:19.296786Z","shell.execute_reply":"2024-10-31T22:36:53.415727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results\n\nFor the model with a learning rate of 2e-4, both of the generator losses decrease across the 25 epochs. The discriminator losses increase in early epochs and then decrease.  \nFor the model with a learning rate of 1e-4, initial generator losses are higher than the previous model, but they rapidly decrease. The discriminator losses are similar to those of the previous model.  \nFor the model with a learning rate of 4e-4,   \nThe table below summarizes the losses for each of the models.\n\n","metadata":{}},{"cell_type":"markdown","source":"| Learning Rate    | Monet Generator | Photo Generator | Monet Discriminator | Photo Discriminator |\n| ---------------- | --------------- | --------------- | ------------------- | ------------------- |\n| 2e-4  | 5.262 | 5.382 | 0.567 | 0.543 |\n| 1e-4 | 5.591 | 5.657 | 0.570 | 0.558 |\n| 4e-4 | 4.899 | 4.997 | 0.595 | 0.552 |","metadata":{}},{"cell_type":"markdown","source":"## Sample Monet-style Photos","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-style\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:01:49.049608Z","iopub.execute_input":"2024-10-31T21:01:49.049916Z","iopub.status.idle":"2024-10-31T21:01:51.897603Z","shell.execute_reply.started":"2024-10-31T21:01:49.049888Z","shell.execute_reply":"2024-10-31T21:01:51.896733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:01:51.899116Z","iopub.execute_input":"2024-10-31T21:01:51.899830Z","iopub.status.idle":"2024-10-31T21:01:52.969762Z","shell.execute_reply.started":"2024-10-31T21:01:51.899789Z","shell.execute_reply":"2024-10-31T21:01:52.968345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:01:52.971519Z","iopub.execute_input":"2024-10-31T21:01:52.971877Z","iopub.status.idle":"2024-10-31T21:07:53.015867Z","shell.execute_reply.started":"2024-10-31T21:01:52.971843Z","shell.execute_reply":"2024-10-31T21:07:53.014843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:07:53.017796Z","iopub.execute_input":"2024-10-31T21:07:53.018133Z","iopub.status.idle":"2024-10-31T21:07:57.441968Z","shell.execute_reply.started":"2024-10-31T21:07:53.018101Z","shell.execute_reply":"2024-10-31T21:07:57.441022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nFor the project, I used a Cycle GAN to translate photographs into Monet-style paintings. The Cycle GAN was constructed from U-Net-based generator, which contracts and expands the images fed to it, and a CNN-based discriminator. Based on informal experimentation, I elected to use Leaky ReLU activation functions for both the downsampling and upsampling functions I compared 3 learning rates for training the model and selected the largest learning rate of 4e-4, which resulted in lower losses for both the generators and discriminators. As seen in the sample photos, the model is able to produce Monet-style paintings when given photographs.  \nFurther experimentation with generator and discriminator architectures may yield better performance.","metadata":{}},{"cell_type":"markdown","source":"# References\nhttps://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\nhttps://medium.com/@william.op.cable/cycle-gans-and-monet-style-transfer-28c0cc1dede6\n","metadata":{}}]}